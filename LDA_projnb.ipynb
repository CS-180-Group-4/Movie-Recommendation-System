{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "movies = pd.read_csv('./dataset/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2969142029.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    vectorizer = CountVectorizer(tokenizer=custom_tokenizer stop_words=combined_stop_words, min_df=100, max_df=0.06)\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "movies.dropna(inplace=True, subset=['description'])\n",
    "\n",
    "# Custom tokenizer function\n",
    "def remove_numbers_and_roman_numerals(text):\n",
    "    number_pattern = r'\\b\\d+\\b'\n",
    "    roman_pattern = r'\\bM{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\b'\n",
    "    text = re.sub(number_pattern, '', text)\n",
    "    text = re.sub(roman_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    text = remove_numbers_and_roman_numerals(text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "custom_stop_words = {'lives', 'life', 'director', 'directed', 'film', 'films', 'filmaker',}\n",
    "combined_stop_words = list(ENGLISH_STOP_WORDS.union(custom_stop_words))\n",
    "\n",
    "# Tokenizing descriptions for dictionary\n",
    "tokenized_descriptions = [doc.split() for doc in movies['description']]\n",
    "\n",
    "# Remove stop words and vectorize to bag of words\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer, stop_words=combined_stop_words, min_df=100, max_df=0.06)\n",
    "X = vectorizer.fit_transform(movies['description'])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to Gensim's corpus\n",
    "corpus = Sparse2Corpus(X, documents_columns=False)\n",
    "id2word = Dictionary(tokenized_descriptions)\n",
    "\n",
    "# Normalize data\n",
    "normalizer = Normalizer()\n",
    "X_normalized = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determining Range of Topics for LDA using Log Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_likelihoods = []\n",
    "# n_topics_options = range(5, 31)  # Trying from 5 to 30 topics\n",
    "# for n_topics in n_topics_options:\n",
    "#     print(\"Starting LDA\", n_topics)\n",
    "#     lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "#     print(\"Model insantiated\")\n",
    "#     lda.fit(X_normalized)\n",
    "#     print(\"Model fitted\")\n",
    "#     ldaScore = lda.score(X_normalized)\n",
    "#     log_likelihoods.append(ldaScore)\n",
    "#     print(\"LDA\", n_topics, \":\", ldaScore)\n",
    "\n",
    "# # Plot log likelihoods\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(n_topics_options, log_likelihoods, marker='o')\n",
    "# plt.xlabel('Number of Topics')\n",
    "# plt.ylabel('Log Likelihood')\n",
    "# plt.title('Log Likelihood by Number of Topics')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determining Model with highest Coherence score from range of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 6\n",
    "\n",
    "print(\"Starting\", n_topics)\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, passes=10, random_state=0)\n",
    "print(\"Model instantiated\")\n",
    "\n",
    "# Calculate coherence score\n",
    "cm = CoherenceModel(model=lda_model, texts=tokenized_descriptions, dictionary=id2word, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "print(\"LDA\", n_topics, \":\", coherence)\n",
    "\n",
    "# Print top words per topic\n",
    "def print_top_words(model):\n",
    "    for idx, topic in model.print_topics(-1):\n",
    "        print(f\"Topic #{idx}: {topic}\")\n",
    "\n",
    "print(\"Top words per topic:\")\n",
    "print_top_words(lda_model)\n",
    "\n",
    "# Select the model with the highest coherence score\n",
    "\n",
    "def convert_lda_output_to_matrix(lda_output, num_topics):\n",
    "    matrix = np.zeros((len(lda_output), num_topics))\n",
    "    for i, doc in enumerate(lda_output):\n",
    "        for topic, prob in doc:\n",
    "            matrix[i, topic] = prob\n",
    "    return matrix\n",
    "\n",
    "lda2d = convert_lda_output_to_matrix(lda_model, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting to K-means Model and Determining Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette_scores = []\n",
    "# inertia_scores = []\n",
    "# for k in range(2, 11):\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "#     kmeans.fit(lda2d)\n",
    "#     score = silhouette_score(lda2d, kmeans.labels_)\n",
    "#     inertia = kmeans.inertia_\n",
    "#     inertia_scores.append(inertia)\n",
    "#     silhouette_scores.append(score)\n",
    "\n",
    "# # Plot silhouette scores\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.title('Silhouette Score vs Number of Clusters')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot silhouette scores\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(range(2, 11), inertia_scores, marker='o')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.title('Silhouette Score vs Number of Clusters')\n",
    "# plt.show()\n",
    "\n",
    "# Select the optimal number of clusters\n",
    "optimal_clusters = 18\n",
    "kmeans_final = KMeans(n_clusters=optimal_clusters, random_state=0)\n",
    "kmeans_final.fit(lda2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['cluster'] = kmeans_final.labels_\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(movies['Cluster'], bins=optimal_clusters, alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.title('Distribution of Movies Across Clusters')\n",
    "plt.xticks(range(optimal_clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words_and_movies_per_cluster(kmeans, feature_names, movies, top_x):\n",
    "    for cluster in range(kmeans.n_clusters):\n",
    "        print(f\"Cluster {cluster}:\")\n",
    "        \n",
    "        # Print top words\n",
    "        cluster_center = kmeans.cluster_centers_[cluster]\n",
    "        top_word_indices = cluster_center.argsort()[:-top_x - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        print(\"Top words:\", \", \".join(top_words))\n",
    "        \n",
    "        # Print top movies\n",
    "        cluster_movies = movies[movies['cluster'] == cluster]\n",
    "        top_movies = cluster_movies.nlargest(top_x, 'rating')[['name', 'rating']]\n",
    "        print(\"Top movies:\")\n",
    "        for idx, row in top_movies.iterrows():\n",
    "            print(f\"Title: {row['name']}, Rating: {row['rating']}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "# Print the top 10 words and top 10 movies in each cluster\n",
    "print_top_words_and_movies_per_cluster(kmeans_final, vocab, movies, top_x=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
