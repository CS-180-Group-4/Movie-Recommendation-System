{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "movies = pd.read_csv('./dataset/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.dropna(inplace=True, subset=['description'])\n",
    "\n",
    "custom_stop_words = {'lives', 'life', 'director', 'directed', 'film', 'films', 'filmaker',}\n",
    "combined_stop_words = list(ENGLISH_STOP_WORDS.union(custom_stop_words))\n",
    "\n",
    "# Tokenizing descriptions for dictionary\n",
    "tokenized_descriptions = [doc.split() for doc in movies['description']]\n",
    "\n",
    "# Remove stop words and vectorize to bag of words\n",
    "vectorizer = CountVectorizer(stop_words=combined_stop_words, min_df=1000, max_df=0.05)\n",
    "X = vectorizer.fit_transform(movies['description'])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to Gensim's corpus\n",
    "corpus = Sparse2Corpus(X, documents_columns=False)\n",
    "id2word = Dictionary(tokenized_descriptions)\n",
    "\n",
    "# Normalize data\n",
    "normalizer = Normalizer()\n",
    "X_normalized = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determining Range of Topics for LDA using Log Likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_likelihoods = []\n",
    "# n_topics_options = range(5, 31)  # Trying from 5 to 30 topics\n",
    "# for n_topics in n_topics_options:\n",
    "#     print(\"Starting LDA\", n_topics)\n",
    "#     lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "#     print(\"Model insantiated\")\n",
    "#     lda.fit(X_normalized)\n",
    "#     print(\"Model fitted\")\n",
    "#     ldaScore = lda.score(X_normalized)\n",
    "#     log_likelihoods.append(ldaScore)\n",
    "#     print(\"LDA\", n_topics, \":\", ldaScore)\n",
    "\n",
    "# # Plot log likelihoods\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(n_topics_options, log_likelihoods, marker='o')\n",
    "# plt.xlabel('Number of Topics')\n",
    "# plt.ylabel('Log Likelihood')\n",
    "# plt.title('Log Likelihood by Number of Topics')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determining Model with highest Coherence score from range of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 6\n",
      "Model instantiated\n",
      "LDA 6 : 0.4531926381654782\n",
      "Top words per topic:\n",
      "Topic #0: 0.025*\"cocky\" + 0.017*\"emotions\" + 0.015*\"creature\" + 0.012*\"recall\" + 0.010*\"Asgardian\" + 0.008*\"In\" + 0.008*\"as\" + 0.008*\"company,\" + 0.007*\"auditions\" + 0.007*\"America\"\n",
      "Topic #1: 0.020*\"Imperial\" + 0.011*\"Asgardian\" + 0.010*\"case\" + 0.009*\"this\" + 0.008*\"loaded\" + 0.007*\"bars.\" + 0.007*\"undergrad\" + 0.007*\"sheriff\" + 0.006*\"begins\" + 0.006*\"collective\"\n",
      "Topic #2: 0.009*\"Saturday\" + 0.008*\"Elio\" + 0.007*\"hilarious\" + 0.007*\"alter\" + 0.007*\"rekindling\" + 0.007*\"Fellowship\" + 0.006*\"stunning\" + 0.006*\"fine\" + 0.005*\"Han\" + 0.005*\"Ronan\"\n",
      "Topic #3: 0.009*\"Games.\" + 0.008*\"Tributes\" + 0.006*\"discovery.\" + 0.005*\"revenge.\" + 0.005*\"son\" + 0.005*\"turned\" + 0.005*\"positive,\" + 0.005*\"governments\" + 0.004*\"skilled\" + 0.004*\"plans,\"\n",
      "Topic #4: 0.012*\"addiction\" + 0.009*\"chickens\" + 0.009*\"anxiety\" + 0.008*\"following\" + 0.007*\"24-hour-a-day\" + 0.007*\"Lupin\" + 0.006*\"no\" + 0.006*\"one\" + 0.006*\"call\" + 0.005*\"land\"\n",
      "Topic #5: 0.014*\"programming,\" + 0.013*\"Midwest\" + 0.010*\"fine\" + 0.009*\"Deckard,\" + 0.009*\"everything\" + 0.008*\"Pan\" + 0.008*\"Torrance\" + 0.007*\"Wizardry.\" + 0.007*\"visions\" + 0.006*\"Kat:\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'LdaModel' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m             matrix[i, topic] \u001b[38;5;241m=\u001b[39m prob\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n\u001b[1;32m---> 29\u001b[0m lda2d \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_lda_output_to_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mconvert_lda_output_to_matrix\u001b[1;34m(lda_output, num_topics)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_lda_output_to_matrix\u001b[39m(lda_output, num_topics):\n\u001b[1;32m---> 23\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlda_output\u001b[49m\u001b[43m)\u001b[49m, num_topics))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lda_output):\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m topic, prob \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'LdaModel' has no len()"
     ]
    }
   ],
   "source": [
    "n_topics = 6\n",
    "\n",
    "print(\"Starting\", n_topics)\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, passes=20, random_state=0)\n",
    "print(\"Model instantiated\")\n",
    "\n",
    "# Calculate coherence score\n",
    "cm = CoherenceModel(model=lda_model, texts=tokenized_descriptions, dictionary=id2word, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "print(\"LDA\", n_topics, \":\", coherence)\n",
    "\n",
    "# Print top words per topic\n",
    "def print_top_words(model):\n",
    "    for idx, topic in model.print_topics(-1):\n",
    "        print(f\"Topic #{idx}: {topic}\")\n",
    "\n",
    "print(\"Top words per topic:\")\n",
    "print_top_words(lda_model)\n",
    "\n",
    "# Select the model with the highest coherence score\n",
    "\n",
    "def convert_lda_output_to_matrix(lda_output, num_topics):\n",
    "    matrix = np.zeros((len(lda_output), num_topics))\n",
    "    for i, doc in enumerate(lda_output):\n",
    "        for topic, prob in doc:\n",
    "            matrix[i, topic] = prob\n",
    "    return matrix\n",
    "\n",
    "lda2d = convert_lda_output_to_matrix(lda_model, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting to K-means Model and Determining Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. KMeans expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m      3\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     score \u001b[38;5;241m=\u001b[39m silhouette_score(lda_transformed, kmeans\u001b[38;5;241m.\u001b[39mlabels_)\n\u001b[1;32m      6\u001b[0m     silhouette_scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:1481\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1492\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1043\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1041\u001b[0m     )\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1049\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1050\u001b[0m         array,\n\u001b[1;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1054\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. KMeans expected <= 2."
     ]
    }
   ],
   "source": [
    "silhouette_scores = []\n",
    "inertia_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(lda2d)\n",
    "    score = silhouette_score(lda2d, kmeans.labels_)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertia_scores.append(inertia)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(2, 11), inertia_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# # Select the optimal number of clusters\n",
    "# optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "# kmeans_final = KMeans(n_clusters=optimal_clusters, random_state=0)\n",
    "# kmeans_final.fit(lda2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['Cluster'] = kmeans_final.labels_\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(movies['Cluster'], bins=optimal_clusters, alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.title('Distribution of Movies Across Clusters')\n",
    "plt.xticks(range(optimal_clusters))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
